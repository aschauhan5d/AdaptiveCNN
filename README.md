
# Adaptive CNN â€“ Automatically Growing Neural Network

## ğŸ” Overview
This project implements an **Adaptive Convolutional Neural Network (Adaptive CNN)** that **automatically increases its depth**
based on validation performance during training.

Unlike traditional CNNs with fixed architectures, this model **dynamically grows** when learning stagnates, making it a
lightweight and research-inspired alternative to Neural Architecture Search (NAS).

---

## ğŸš€ Key Features
- Dynamic CNN architecture (auto layer growth)
- Validation-lossâ€“driven decision making
- Simple AutoML-inspired design
- Clean, research-ready implementation

---

## ğŸ§  Core Idea
1. Start with a shallow CNN  
2. Train for a few epochs  
3. Monitor validation loss  
4. If improvement < threshold â†’ add Conv layer  
5. Repeat until max depth or convergence  

---

## ğŸ“ Repository Structure
```
adaptive-cnn/
â”‚â”€â”€ model/
â”‚   â””â”€â”€ adaptive_model.py
â”‚â”€â”€ train.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ results/
â”‚â”€â”€ README.md
```

---

## ğŸ“¦ Dataset
- **CIFAR-10** (automatically downloaded via TensorFlow)

---

## ğŸ› ï¸ Installation
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Run Training
```bash
python train.py
```

---

## ğŸ“Š Sample Output
```
Step 1 | Val Loss: 1.25
Step 2 | Val Loss: 1.04
No improvement â†’ Adding new Conv layer
Step 3 | Val Loss: 0.97
```

---

## ğŸ“Œ Research Relevance
- AutoML concepts
- Adaptive systems
- Model capacity control
- Neural architecture exploration

---

## ğŸ‘¤ Author
Abhishek Singh Chauhan  
MCA (AI/ML)

---

## ğŸ“œ License
MIT License
